{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from scipy.special import expit\n",
    "from time import sleep\n",
    "from sklearn.model_selection import ParameterGrid, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from skbayes.rvm_ard_models import RVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warning generated by classifier libraries.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyConfusionMatrix(Y,ClassNames):\n",
    "    print(confusion_matrix(ClassNames, Y))\n",
    "    return confusion_matrix(ClassNames, Y),accuracy_score(ClassNames, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svc(parameters):\n",
    "    svc = SVC(C=parameters['parameters']['C'], kernel=parameters['parameters']['kernel'],\n",
    "              degree=parameters['parameters']['degree'], gamma=parameters['parameters']['gamma'],\n",
    "              coef0=parameters['parameters']['coef0'], probability=parameters['parameters']['probability'],\n",
    "              tol=parameters['parameters']['tol'], cache_size=parameters['parameters']['cache_size'],\n",
    "              class_weight=parameters['parameters']['class_weight'], shrinking=parameters['parameters']['shrinking'],\n",
    "              verbose=parameters['parameters']['verbose'], max_iter=parameters['parameters']['max_iter'],\n",
    "              decision_function_shape=parameters['parameters']['decision_function_shape'],\n",
    "              random_state=parameters['parameters']['random_state'])\n",
    "    return svc\n",
    "\n",
    "\n",
    "def svc_train(svc, estimate, labels):\n",
    "    svc.fit(estimate, labels)\n",
    "\n",
    "\n",
    "def svc_predict(svc, validate):\n",
    "    return svc.predict(validate)\n",
    "\n",
    "\n",
    "def svc_probability(svc, validate):\n",
    "    return svc.predict_proba(validate)\n",
    "\n",
    "\n",
    "def svc_score(svc, validate, v_labels):\n",
    "    return svc.score(validate, v_labels)\n",
    "\n",
    "\n",
    "def svc_get_para(svc):\n",
    "    support = svc.support_\n",
    "    support_vectors = svc.support_vectors_\n",
    "    n_support = svc.n_support_\n",
    "    dual_coef = svc.dual_coef_\n",
    "    intercept = svc.intercept_\n",
    "    sparse = svc._sparse\n",
    "    shape_fit = svc.shape_fit_\n",
    "    prob_a = svc.probA_\n",
    "    prob_b = svc.probB_\n",
    "    gamma = svc._gamma\n",
    "    classes = svc.classes_\n",
    "    hyper = svc.get_params(deep=True)\n",
    "    ret = {'support': support, 'support_vectors': support_vectors, 'n_support': n_support, 'dual_coef': dual_coef,\n",
    "           'intercept': intercept, 'sparse': sparse, 'shape_fit': shape_fit, 'prob_a': prob_a, 'prob_b': prob_b,\n",
    "           'gamma': gamma, 'classes': classes, 'hyper': hyper}\n",
    "    return ret\n",
    "\n",
    "\n",
    "def svc_set_para(svc, svc_para):\n",
    "    svc.set_params(**svc_para['hyper'])\n",
    "    svc.support_ = svc_para['support']\n",
    "    svc.support_vectors_ = svc_para['support_vectors']\n",
    "    svc.n_support_ = svc_para['n_support']\n",
    "    svc._dual_coef_ = svc_para['dual_coef']\n",
    "    svc._intercept_ = svc_para['intercept']\n",
    "    svc._sparse = svc_para['sparse']\n",
    "    svc.shape_fit_ = svc_para['shape_fit']\n",
    "    svc.probA_ = svc_para['prob_a']\n",
    "    svc.probB_ = svc_para['prob_b']\n",
    "    svc._gamma = svc_para['gamma']\n",
    "    svc.classes_ = svc_para['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMyClassifier(XEstimate, ClassLabels, XValidate, Parameters ):\n",
    "    if Parameters['algorithm'] == 'RVM':\n",
    "        Parameters = Parameters['parameters']\n",
    "        clf = RVC(n_iter = Parameters.get('n_iter'), tol = Parameters.get('tol'),\n",
    "                    n_iter_solver = Parameters.get('n_iter_solver'), tol_solver = Parameters.get('tol_solver'),\n",
    "                    fit_intercept = Parameters.get('fit_intercept'),\n",
    "                    verbose = Parameters.get('verbose'),\n",
    "                    kernel = Parameters.get('kernel'),\n",
    "                    degree = Parameters.get('degree'),\n",
    "                    gamma = Parameters.get('gamma'),\n",
    "                    coef0 = Parameters.get('coef0'),\n",
    "                    kernel_params = Parameters.get('kernel_params') )\n",
    "    \n",
    "        clf.fit(XEstimate, ClassLabels)\n",
    "        prob = clf.predict_proba(XValidate)\n",
    "        \n",
    "        prob_std = np.ndarray.std(prob, axis=1)[:, np.newaxis]\n",
    "        sigmoid = 1 - expit(prob_std)\n",
    "        Yvalidate = np.concatenate([prob, sigmoid], axis=1)\n",
    "        Yvalidate = Yvalidate / np.repeat((sigmoid + 1), axis=1, repeats = np.shape(clf.classes_)[0] + 1)\n",
    "        \n",
    "        EstParameters = { 'relevant_vectors':clf.relevant_vectors_ ,'coef':clf.coef_,'active':clf.active_,\n",
    "                          'intercept':clf.intercept_,'mean':clf._x_mean, 'std':clf._x_std,'classes':clf.classes_,\n",
    "                          'lambda':clf.lambda_,'sigma':clf.sigma_,'relevant':clf.relevant_}\n",
    "        \n",
    "        return Yvalidate, EstParameters\n",
    "\n",
    "    elif Parameters['algorithm'] == 'SVM':\n",
    "        \n",
    "        svc = get_svc(Parameters)\n",
    "        svc_train(svc, XEstimate, ClassLabels)\n",
    "        prob = svc_probability(svc, XValidate)\n",
    "        EstParameters = svc_get_para(svc)\n",
    "        \n",
    "        prob_std = np.ndarray.std(prob, axis=1)[:, np.newaxis]\n",
    "        sigmoid = 1 - expit(prob_std)\n",
    "        Yvalidate = np.concatenate([prob, sigmoid], axis=1)\n",
    "        Yvalidate = Yvalidate / np.repeat((sigmoid + 1), axis=1, repeats=len(svc.classes_)+1)\n",
    "        \n",
    "        return Yvalidate, EstParameters\n",
    "\n",
    "    elif Parameters[\"algorithm\"] == \"GPR\":\n",
    "        # get the classes from the labels\n",
    "        classes = np.unique(ClassLabels, axis=0)\n",
    "        sorted(classes, reverse=True)\n",
    "        num_class = len(classes)\n",
    "\n",
    "        # get data and label based on classes\n",
    "        data = []\n",
    "        for cla in classes:\n",
    "            data.append(XEstimate[ClassLabels == cla])\n",
    "\n",
    "        target = []\n",
    "        for cla in classes:\n",
    "            target.append(ClassLabels[ClassLabels == cla])\n",
    "\n",
    "        # put data and label into a matrix, so that we could do a easier calculation for probability\n",
    "        # the following calculation is all based on the matrix\n",
    "        data_matrix = []\n",
    "        for i in range(num_class - 1):\n",
    "            data_matrix.append([])\n",
    "            for j in range(num_class - 1):\n",
    "                data_matrix[i].append(None)\n",
    "\n",
    "        target_matrix = []\n",
    "        for i in range(num_class - 1):\n",
    "            target_matrix.append([])\n",
    "            for j in range(num_class - 1):\n",
    "                target_matrix[i].append(None)\n",
    "\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                data_matrix[i][j] = np.concatenate([data[i], data[j+1]], axis=0)\n",
    "                target_matrix[i][j] = np.concatenate([target[i], target[j+1]], axis=0)\n",
    "\n",
    "        classifier_matrix = []\n",
    "        for i in range(num_class-1):\n",
    "            classifier_matrix.append([])\n",
    "            for j in range(num_class-1):\n",
    "                classifier_matrix[i].append(None)\n",
    "\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                gpc_classifier = GaussianProcessClassifier(\n",
    "                    kernel=Parameters[\"parameters\"][\"kernel\"],\n",
    "                    optimizer=Parameters[\"parameters\"][\"optimizer\"],\n",
    "                    n_restarts_optimizer=Parameters[\"parameters\"][\"n_restarts_optimizer\"],\n",
    "                    max_iter_predict=Parameters[\"parameters\"][\"max_iter_predict\"],\n",
    "                    warm_start=Parameters[\"parameters\"][\"warm_start\"],\n",
    "                    copy_X_train=Parameters[\"parameters\"][\"copy_X_train\"],\n",
    "                    random_state=Parameters[\"parameters\"][\"random_state\"],\n",
    "                    multi_class=\"one_vs_rest\",\n",
    "                    n_jobs=Parameters[\"parameters\"][\"n_jobs\"]\n",
    "                )\n",
    "                gpc_classifier.fit(data_matrix[i][j], target_matrix[i][j])\n",
    "                classifier_matrix[i][j] = gpc_classifier\n",
    "\n",
    "        out_matrix = []\n",
    "        for i in range(num_class-1):\n",
    "            out_matrix.append([])\n",
    "            for j in range(num_class-1):\n",
    "                out_matrix[i].append(None)\n",
    "\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                out_matrix[i][j] = classifier_matrix[i][j].predict_proba(XValidate)\n",
    "\n",
    "        # calculate the whole prediction prob\n",
    "        val_shape = XValidate.shape[0]\n",
    "        predict_prob_list = []\n",
    "        for i in range(num_class):\n",
    "            predict_prob_list.append(np.zeros(shape=[val_shape, 1]))\n",
    "\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                predict_prob_list[i] += out_matrix[i][j][:, 0][:, np.newaxis] / (num_class * 2)\n",
    "                predict_prob_list[j + 1] += out_matrix[i][j][:, 1][:, np.newaxis] / (num_class * 2)\n",
    "\n",
    "        # get the result of num_class probability\n",
    "        result = np.concatenate(predict_prob_list, axis=1)\n",
    "\n",
    "        # calculate the probability for the one more class\n",
    "        std = np.std(result, axis=1)[:, np.newaxis]\n",
    "        other_prob = np.exp(-std) / (1 + np.exp(std * 5))\n",
    "        result = np.concatenate([result, other_prob], axis=1)\n",
    "        result = result / np.repeat((other_prob + 1), axis=1, repeats=num_class + 1)\n",
    "\n",
    "        # put all the parameters into a dict\n",
    "        estParameters = {}\n",
    "        estParameters[\"class_num\"] = num_class\n",
    "        estParameters[\"parameters\"] = []\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                estParameters[\"parameters\"].append(\n",
    "                    {\n",
    "                        \"log_marginal_likelihood_value_\": classifier_matrix[i][j].log_marginal_likelihood_value_,\n",
    "                        \"classes_\": classifier_matrix[i][j].classes_,\n",
    "                        \"n_classes_\": classifier_matrix[i][j].n_classes_,\n",
    "                        \"base_estimator_\": classifier_matrix[i][j].base_estimator_\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return result, estParameters\n",
    "\n",
    "def CVhelper(args):\n",
    "    XEstimate=args[0]\n",
    "    XValidate= args[1]\n",
    "    YEstimate=args[2]\n",
    "    YValidateTrue=args[3]\n",
    "    algo=args[4]\n",
    "    params=args[5]\n",
    "                \n",
    "    # calling TrainMyClassifier\n",
    "    YValidate, Estparameters = TrainMyClassifier(XEstimate, YEstimate, XValidate,\n",
    "                                                 {'algorithm':algo, 'parameters':params })\n",
    "                \n",
    "                \n",
    "    score = (YValidateTrue == np.sum(np.argmax(YValidate, axis=1))/np.size(YValidateTrue))*100\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyCrossValidate(XTrain, ClassLabels, Nf):\n",
    "    \"\"\"\n",
    "    Xtrain: Training data with labels\n",
    "    ClassLabels: Class labels for train set.\n",
    "    Nf: Number of folds\n",
    "    \n",
    "    returns:\n",
    "            Array of Ytrain:\n",
    "            Array of EstParameters:\n",
    "            Array of EstConfMatrices:\n",
    "            Array of ConfMatrix:\n",
    "    \"\"\"\n",
    "    \n",
    "    algorithms = ['SVM', 'RVM', 'GPR']\n",
    "    parameters = {'SVM':{'C' : [1, 5, 10], 'kernel' : ['rbf','poly'], 'degree' : [2, 3, 5], 'gamma' : ['auto'],\n",
    "                            'coef0' : [0.0], 'probability' : [True], 'shrinking' : [True], 'tol' : [1e-3, 1e-4],\n",
    "                            'class_weight' : ['balanced'], 'verbose' : [False], 'max_iter' : [-1],\n",
    "                            'decision_function_shape' : ['ovo'], 'random_state' : [None], 'cache_size': [800]},\n",
    "                  \n",
    "                 'RVM':{ 'n_iter':[10, 20, 30], 'tol':[1e-4], 'n_iter_solver':[15], 'tol_solver':[1e-4],\n",
    "                 'fit_intercept':[True], 'verbose': [False], 'kernel':['rbf'], 'degree': [2],\n",
    "                 'gamma': [None], 'coef0':[1], 'kernel_params':[None]},\n",
    "                 'GPR':{\"kernel\": [ 1.0 * RBF(1), \n",
    "                                    2.0 * RBF(1),\n",
    "                                    3.0 * RBF(1),\n",
    "                                    1.0 * RBF(2),\n",
    "                                    1.0 * RBF(3)\n",
    "                                    ],\n",
    "                              \"optimizer\": [\"fmin_l_bfgs_b\"],\n",
    "                              \"n_restarts_optimizer\": [0],\n",
    "                              \"max_iter_predict\": [100],\n",
    "                              \"warm_start\": [True],\n",
    "                              \"copy_X_train\": [True],\n",
    "                              \"random_state\": [0],\n",
    "                              \"multi_class\": [\"one_vs_one\"],\n",
    "                              \"n_jobs\": [-1]}}\n",
    "    # de code ClassLabels to numbers \n",
    "    ClassLabels = np.argmax(ClassLabels, axis=1)\n",
    "    \n",
    "    \n",
    "    algo_score = []\n",
    "    algo_params = []\n",
    "    \n",
    "    Ytrain = []\n",
    "    EstParameter = []\n",
    "    EstConfMatrices = []\n",
    "    ConfMatrix = []\n",
    "    for algo in algorithms:\n",
    "        \n",
    "        # generating hyper parameter array for hyper parameter search.\n",
    "        grid = ParameterGrid(parameters[algo])\n",
    "        grid_search_score = []\n",
    "        pbar = tqdm(list(grid))\n",
    "        for params in pbar:\n",
    "            pbar.set_description(\"Searching Parameters for {}\".format(algo))\n",
    "            # scikit-learn object to divide data set in Estimate and Validate sets.\n",
    "            k_fold = KFold(n_splits=Nf, random_state=None, shuffle=False)\n",
    "            \n",
    "            # Array of scores for each split.\n",
    "            cv_scores = []\n",
    "            p = Pool(5)\n",
    "            p_params =[]\n",
    "            for train_index, val_index in k_fold.split(XTrain):\n",
    "\n",
    "                # Spliting data in Estimate and validate set.\n",
    "                XEstimate, XValidate = XTrain[train_index], XTrain[val_index]\n",
    "                YEstimate, YValidateTrue = ClassLabels[train_index], ClassLabels[val_index]\n",
    "                p_params.append([XEstimate, XValidate, YEstimate, YValidateTrue, algo, params])\n",
    "            \n",
    "            cv_scores = p.map(CVhelper, p_params)\n",
    "            \n",
    "            # average accuracy for selected hyper parameters \n",
    "            score = np.mean(cv_scores)\n",
    "            grid_search_score.append(score)\n",
    "\n",
    "        # calculating best hyper parameters\n",
    "        idx = np.argmax(grid_search_score)\n",
    "        best_params = list(grid)[idx]\n",
    "        \n",
    "        # storing best algorithm score and hyper parameters\n",
    "        algo_score.append(np.max(grid_search_score))\n",
    "        algo_params.append(best_params)\n",
    "        \n",
    "        \n",
    "        # calculating method out for current algorithm\n",
    "        _Ytrain = []\n",
    "        _EstParameter = []\n",
    "        _EstConfMatrices = []\n",
    "        _ConfMatrix = []\n",
    "\n",
    "        # scikit-learn object to divide data set in Estimate and Validate sets.\n",
    "        k_fold = KFold(n_splits=Nf, random_state=None, shuffle=False)\n",
    "\n",
    "        _all_Yval_true = []\n",
    "        _all_Yval_pred = []\n",
    "        index = 0\n",
    "        \n",
    "        print(\"Average cross validation score for {} is {}%.\".format(algo, np.max(grid_search_score)))\n",
    "        for train_index, val_index in k_fold.split(XTrain):\n",
    "\n",
    "            # Spliting data in Estimate and validate set.\n",
    "            XEstimate, XValidate = XTrain[train_index], XTrain[val_index]\n",
    "            YEstimate, YValidateTrue = ClassLabels[train_index], ClassLabels[val_index]\n",
    "\n",
    "            # calling TrainMyClassifier\n",
    "            YValidate, Estparameters = TrainMyClassifier(XEstimate, YEstimate, XValidate,\n",
    "                                                         {'algorithm':algo, 'parameters':best_params })\n",
    "            \n",
    "            # storing TrainMyClassifier's output  \n",
    "            _Ytrain.append(YValidate)\n",
    "            _EstParameter.append(Estparameters)\n",
    "            \n",
    "            # printing kernel numbers \n",
    "            if  algo == 'SVM':\n",
    "                print('Number of support vector for validation set {}: {}'.format(index, len(Estparameters['support_vectors'])))\n",
    "            elif  algo == 'RVM':\n",
    "                print('Number of relavance vector for validation set {}: {}'.format(index, len(Estparameters['relevant_vectors'])))\n",
    "                \n",
    "            # calculating confusion matrix for validation set\n",
    "            print(\"Confusion matrix for validation set {}: \".format(index))\n",
    "            _EstConfMatrices.append(MyConfusionMatrix(np.argmax(YValidate, axis=1), YValidateTrue))\n",
    "            \n",
    "            \n",
    "            _all_Yval_true.extend(YValidateTrue.tolist())\n",
    "            _all_Yval_pred.extend(np.argmax(YValidate, axis=1).tolist())\n",
    "            \n",
    "            index+=1\n",
    "        print(\"Over all Confusion matrix for {}: \".format(algo))\n",
    "        # calculating over all confusion matrix for validation set\n",
    "        _all_Yval_pred = np.array(_all_Yval_pred).reshape(len(_all_Yval_pred),1)\n",
    "        _all_Yval_true = np.array(_all_Yval_true).reshape(len(_all_Yval_true),1)\n",
    "        _ConfMatrix = MyConfusionMatrix(_all_Yval_pred, _all_Yval_true)\n",
    "        \n",
    "        # append Main outputs\n",
    "        Ytrain.append(_Ytrain)\n",
    "        EstParameter.append(_EstParameter)\n",
    "        EstConfMatrices.append(_EstConfMatrices)\n",
    "        ConfMatrix.append(_ConfMatrix)\n",
    "        \n",
    "        # waiting for all print commands to execute.\n",
    "        sleep(.5)\n",
    "        \n",
    "    return Ytrain, EstParameter, EstConfMatrices, ConfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = io.loadmat(\"Proj2FeatVecsSet1.mat\")['Proj2FeatVecsSet1']\n",
    "# label shape is [25000, 5] as [num_sample, num_class]\n",
    "label = io.loadmat(\"Proj2TargetOutputsSet1.mat\")['Proj2TargetOutputsSet1']\n",
    "data = np.concatenate((train, label), axis=1)\n",
    "np.random.shuffle(data)\n",
    "train = data[:,:-5]\n",
    "label = data[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Parameters for SVM:  39%|███▉      | 14/36 [1:32:18<2:25:03, 395.60s/it]"
     ]
    }
   ],
   "source": [
    "Ytrain, EstParameter, EstConfMatrices, ConfMatrix = MyCrossValidate(train, label, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestMyClassifier(XTest,Parameters,EstParameters):\n",
    "    if Parameters['algorithm'] == 'SVM':\n",
    "        n_svc = get_svc(Parameters)\n",
    "        svc_set_para(n_svc, EstParameters)\n",
    "        Ytest = svc_probability(n_svc, XTest)\n",
    "        \n",
    "        return Ytest\n",
    "    \n",
    "    elif Parameters['algorithm'] == 'RVM':\n",
    "        \n",
    "        Parameters = Parameters['parameters']\n",
    "        \n",
    "        clf = RVC(n_iter=Parameters.get('n_iter'),\n",
    "              tol=Parameters.get('tol'),\n",
    "              n_iter_solver=Parameters.get('n_iter_solver'),\n",
    "              tol_solver=Parameters.get('tol_solver'),\n",
    "              fit_intercept=Parameters.get('fit_intercept'),\n",
    "              verbose=Parameters.get('verbose'),\n",
    "              kernel=Parameters.get('kernel'),\n",
    "              degree=Parameters.get('degree'),\n",
    "              gamma=Parameters.get('gamma'),\n",
    "              coef0=Parameters.get('coef0'),\n",
    "              kernel_params=Parameters.get('kernel_params'))\n",
    "        \n",
    "        clf.relevant_vectors_ = EstParameters.get('relevant_vectors')\n",
    "        clf.relevant_ = EstParameters.get('relevant')\n",
    "        clf.active_ = EstParameters.get('active')\n",
    "        clf.coef_ = EstParameters.get('coef')\n",
    "        clf.intercept_ = EstParameters.get('intercept')\n",
    "        clf._x_mean = EstParameters.get('mean')\n",
    "        clf._x_std = EstParameters.get('std')\n",
    "        clf.classes_ = EstParameters.get('classes')\n",
    "        clf.lambda_ = EstParameters.get('lambda')\n",
    "        clf.sigma_ = EstParameters.get('sigma')\n",
    "        Ytest = clf.predict_proba(XTest)\n",
    "        \n",
    "        return Ytest\n",
    "\n",
    "    elif Parameters['algorithm'] == 'GPR':\n",
    "        num_class = EstParameters[\"class_num\"]\n",
    "        classifier = []\n",
    "        # init all the classifiers\n",
    "        for param_dict in EstParameters[\"parameters\"]:\n",
    "            gpc_classifier = GaussianProcessClassifier(\n",
    "                kernel=Parameters[\"parameters\"][\"kernel\"],\n",
    "                optimizer=Parameters[\"parameters\"][\"optimizer\"],\n",
    "                n_restarts_optimizer=Parameters[\"parameters\"][\"n_restarts_optimizer\"],\n",
    "                max_iter_predict=Parameters[\"parameters\"][\"max_iter_predict\"],\n",
    "                warm_start=Parameters[\"parameters\"][\"warm_start\"],\n",
    "                copy_X_train=Parameters[\"parameters\"][\"copy_X_train\"],\n",
    "                random_state=Parameters[\"parameters\"][\"random_state\"],\n",
    "                multi_class=\"one_vs_rest\",\n",
    "                n_jobs=Parameters[\"parameters\"][\"n_jobs\"]\n",
    "            )\n",
    "            gpc_classifier.log_marginal_likelihood_value_ = param_dict[\"log_marginal_likelihood_value_\"]\n",
    "            gpc_classifier.classes_ = param_dict[\"classes_\"]\n",
    "            gpc_classifier.n_classes_ = param_dict[\"n_classes_\"]\n",
    "            gpc_classifier.base_estimator_ = param_dict[\"base_estimator_\"]\n",
    "            classifier.append(gpc_classifier)\n",
    "\n",
    "        # put all the classifiers into a matrix, so it is easier for calculation\n",
    "        classifier_matrix = []\n",
    "        for i in range(num_class-1):\n",
    "            classifier_matrix.append([])\n",
    "            for j in range(num_class-1):\n",
    "                classifier_matrix[i].append(None)\n",
    "\n",
    "        count = 0\n",
    "        for i in range(num_class-1):\n",
    "            for j in range(i, num_class-1):\n",
    "                classifier_matrix[i][j] = classifier[count]\n",
    "                count += 1\n",
    "\n",
    "        # calculate the output for XTest\n",
    "        out_matrix = []\n",
    "        for i in range(num_class - 1):\n",
    "            out_matrix.append([])\n",
    "            for j in range(num_class - 1):\n",
    "                out_matrix[i].append(None)\n",
    "\n",
    "        for i in range(num_class - 1):\n",
    "            for j in range(i, num_class - 1):\n",
    "                out_matrix[i][j] = classifier_matrix[i][j].predict_proba(XTest)\n",
    "\n",
    "        # calculate the whole prediction prob\n",
    "        val_shape = XTest.shape[0]\n",
    "        predict_prob_list = []\n",
    "        for i in range(num_class):\n",
    "            predict_prob_list.append(np.zeros(shape=[val_shape, 1]))\n",
    "\n",
    "        for i in range(num_class - 1):\n",
    "            for j in range(i, num_class - 1):\n",
    "                predict_prob_list[i] += out_matrix[i][j][:, 0][:, np.newaxis] / (num_class * 2)\n",
    "                predict_prob_list[j + 1] += out_matrix[i][j][:, 1][:, np.newaxis] / (num_class * 2)\n",
    "\n",
    "        result = np.concatenate(predict_prob_list, axis=1)\n",
    "\n",
    "        # calculate the probability for the one more class\n",
    "        std = np.std(result, axis=1)[:, np.newaxis]\n",
    "        other_prob = np.exp(-std) / (1 + np.exp(std * 5))\n",
    "        result = np.concatenate([result, other_prob], axis=1)\n",
    "        result = result / np.repeat((other_prob + 1), axis=1, repeats=num_class + 1)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([[0.15885083690507712, 0.1578521841507338, 0.15818055651886165, 0.1943885209218602, 0.33072790150346726], [0.15731620902088092, 0.1564886559303819, 0.1571724552852761, 0.19869956766066554, 0.3303231121027957], [0.1571559320460259, 0.15626959611253674, 0.16152356123500736, 0.1943381529159996, 0.33071275769043024], [0.1590957591161467, 0.15738400047332465, 0.15949447566554573, 0.19318779604031705, 0.33083796870466586], [0.15747164329634458, 0.15660593869623493, 0.15751488843432182, 0.19802081435816357, 0.33038671521493507], [0.23897136087275767, 0.19476830515536117, 0.23629961062832178, 0.3299607233435594], [0.2347695966947935, 0.20080752295177864, 0.2337173783766806, 0.3307055019767472], [0.23908334099507736, 0.19480621893924216, 0.23614613858401015, 0.32996430148167044], [0.23906442834245786, 0.19505408307407018, 0.2358879002881175, 0.32999358829535447], [0.238673941528362, 0.19487144729764413, 0.2364795918569872, 0.3299750193170067]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_2)",
   "language": "python",
   "name": "ml_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
